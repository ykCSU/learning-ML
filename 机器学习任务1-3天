<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
1. 了解什么是Machine learning
≈从数据中寻找一个函数
2. a.中心极限定理:
现在我们要统计全国的人的体重，看看我国平均体重是多少。当然，我们把全国所有人的体重都调查一遍是不现实的。所以我们打算一共调查1000组，每组50个人。 然后，我们求出第一组的体重平均值、第二组的体重平均值，一直到最后一组的体重平均值。中心极限定理说：这些平均值是呈现正态分布的。并且，随着组数的增加，效果会越好。 最后，当我们再把1000组算出来的平均值加起来取个平均值，这个平均值会接近全国平均体重。
其中要注意的几点：
总体本身的分布不要求正态分布  
上面的例子中，人的体重是正态分布的。但如果我们的例子是掷一个骰子（平均分布），最后每组的平均值也会组成一个正态分布。（神奇！）
样本每组要足够大，但也不需要太大  
取样本的时候，一般认为，每组大于等于30个，即可让中心极限定理发挥作用。
   b.正态分布:
正态分布（英语：normal distribution）又名高斯分布（英语：Gaussian distribution），是一个非常常见的连续概率分布。正态分布在统计学上十分重要，经常用在自然和社会科学来代表一个不明的随机变量。
$X~N(\mu ,\sigma ^{2})$
则其概率密度函数为
$f(x)=\frac{1}{\sigma \sqrt{2\pi }}e^{-\frac{(x-\mu )^{2}}{2\sigma ^{2}}}$
正态分布的数学期望值或期望值\mu等于位置参数，决定了分布的位置；其方差\sigma^2的开平方或标准差\sigma等于尺度参数，决定了分布的幅度。

正态分布的概率密度函数曲线呈钟形，因此人们又经常称之为钟形曲线（类似于寺庙里的大钟，因此得名）。我们通常所说的标准正态分布是位置参数\mu =0，尺度参数\sigma^2 = 1的正态分布。
    c.最大似然估计:
    https://www.cnblogs.com/lliuye/p/9139032.html
2.1 推导回归Loss function
    $L(f)=\sum_{n=1}^{n=N}(\hat{y}-f(x^{n}))^{2}=\sum_{n=1}^{n=N}(\hat{y}-(b+w*x)))^{2}$,其中$f(x)=y$,$y=b+w*x$
2.2 学习损失函数与凸函数之间的关系
2.3 了解全局最优和局部最优
3. a.学习导数 $\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^{0}}$
   b.泰勒展开 $f(x)=\frac{f(x_{0})}{0!}+\frac{f'(x_{0})}{1!}(x-x_{0})+\frac{f''(x_{0})}{2!}(x-x_{0})^{2}+...+\frac{f^{n}(x_{0})}{n!}(x-x_{0})^{n}+R_{n}(x)$
3.1 推导梯度下降公式
3.2 写出梯度下降的代码
4. $L(x,y)=\sum_{j=1}^{n}(\sqrt{(x_{j}-y_{j})^{2}})^{p}$
这个公式是计算距离，正则化其实就是计算参数到0点的距离，于是$y_{j}=0$，于是$L(x,y)=\sum_{j=1}^{n}(\sqrt{(x_{j})^{2}})^{p}$
，则p=0表示L0正则，p=1表示L1正则，p=2表示L2正则。
a.L0-Norm L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，让参数W是稀疏的。如果要加这个额外项就是如下所示：
$\sum_{j=1,\theta _{j}\neq 0}^{n}$$\theta_{j}^{0}$
上述公式中0的0次方没有意义，所以参数为0的不参与惩罚项，不参与加权，其他参数非0项的0次方都为1，所以这上述公式就是统计参数中非0项的个数。
优点 
L0是比较简单的正则范数，他的目的就是尽可能的让参数中的元素尽可能等于0，可以获得真正sparse的模型。 
缺点 
L0范数很难优化求解（NP难问题），求解复杂度很高。
b.L1-Norm L1范数是指向量中各个元素绝对值之和 $\sum_{j=1}^{n}|\theta _{j}|$
c. L2-Norm  $\sum_{j=1}^{n}|\theta _{j}|^{2}$
优点 
L2让所有特征的系数都缩小，但是不会减为0，这样矩阵并不是稀疏矩阵，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况。 
缺点 
无法获得sparse模型，因为矩阵都不是为0的元素，也无法特征选择。
4.1 优点 
1、L1范数是L0范数的最优凸近似，求解复杂度低，相对于L0范数求解难度降低很多。 
2、能够获得sparse模型，对于large-scale的问题来说这一点很重要，因为可以减少存储空间
缺点 
加入L1后目标函数在原点不可导，需要做特殊处理。
4.2 学习为什么只对w/Θ做限制，不对b做限制: 我们期待一个参数接近于零的函数，比较平滑（输入有变化时对输入不敏感），
而$y=b+\sum (w_{i}x_{i})$,当x变化时，$y+\sum (w_{i}\Delta x_{i})=b+\sum (w_{i}（x_{i}+\Delta x_{i}）)$，而b无影响函数的平滑，只会上下移动。




reference:
https://blog.csdn.net/randompeople/article/details/80388385
